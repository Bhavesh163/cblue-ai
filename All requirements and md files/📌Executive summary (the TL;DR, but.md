ğŸ“ŒExecutive summary

* Development of customer support AI chatbot for a website. The information that the chatbot get to answer is only from or related to website.



* Single-monorepo, modular services (keep /common/ utilities). âœ…



* Model runs as a single long-lived FP32 process on the RTX 3090, It uses OpenThaiGPT 1.5 14B Instruct FP32, runs fully on a local machine (e.g., RTX 3090) with PyTorch 2.5.1+cu124 with CUDA 12.8 on ubuntu22.04 exposed over a local RPC/UNIX socket. Use a queued worker to serialize GPU work (1 active inference at a time, small worker pool 1â€“2). âœ…



* Frontend unchanged (you said keep it). Use the existing React widget with WebSocket/SSE streaming. âœ…



* Retrieval: Hybrid BM25 + FAISS on local embeddings; reindex via admin UI. âœ…



* Safety \& guardrails: RAG-only answers for factual content; explicit disclaimers for legal content; PII redaction and opt-out. âœ…



* Production concerns: rate limits, queue length caps, memory guards, metrics, backups, CI checks, and an emergency fallback model (small distilled CPU model or canned FAQ responses). âœ…










ğŸ“ŒArchitecture



1. Monorepo layout (follow your /cblue-ai/ layout in Mod2). Keep /common/ for reuse.



2\. Runtime components

* Model Service (GPU) â€” single process, OpenThaiGPT 1.5 14B Instruct FP32 (Hugging Face, local) is at \\\\wsl.localhost\\Ubuntu-24.04\\home\\ballhog\\Litigation\_ai\\models\\14b.
* Runs fully local on RTX 3090; FP32 for full precision legal reasoning, loads model once. Exposes gRPC/HTTP over a local UNIX socket (e.g., /var/run/openthaigpt.sock) and supports streaming. Lives in /services/\*/model\_service/ and optionally runs outside Docker (venv) or inside a GPU-enabled container.
* API Backend (FastAPI) â€” web/API endpoints, WebSocket streaming to frontend, auth, job queue client, retrieval client. Runs in Docker.
* Retrieval Service â€” FAISS index store + BM25 local process (can be a module under /common/retrieval/). Reindex via admin UI.
* Queue + Cache â€” Redis for queue (RQ/Redis Streams) and cache (session state). Use Redis to persist job queue and pubsub for streaming updates.
* DB â€” Postgres for audit logs, config, admin content, and versioned docs.
* Reverse Proxy â€” Nginx for TLS, rate limiting, and static assets.
* Observability â€” Prometheus, Grafana, Sentry.
* Admin UI â€” React separate app to manage docs, reindex, review feedback, escalate.





Diagram (conceptual): Frontend â†(WebSocket)â†’ FastAPI â†(Redis queue)â†’ Worker â†(UNIX socket/gRPC)â†’ Model Service (GPU)

Retrieval (FAISS/BM25) is called inside the worker before model call.







ğŸ“ŒKey design rules (simple)



1. Best practices take precedence â€” follow secure CI, tests, monitoring, immutable builds. If conflicts, choose best practice â†’ Mod2 â†’ Mod1.



2\. Single GPU process â€” do NOT spawn per-request model processes. Load once, serve many requests via queue.



3\. Streaming + backpressure â€” stream tokens to client but enforce queue length and per-user rate limits. If queue full, return a human-friendly message and suggest escalation.



4\. Fail-safe fallbacks â€” On OOM or crash: swap to small distilled model (CPU) or serve cached FAQ answers. Donâ€™t pretend to be the big model when itâ€™s unavailable. (Honest bots are cutest.)



(Joke: If the model ever gets stage fright, press Ctrl+Alt+Del â€” for dramatic effect only. ğŸ˜…)







ğŸ“ŒModel serving specifics (practical)



* Load: transformers/custom loader using torch.cuda.set\_per\_process\_memory\_fraction() + with torch.no\_grad(): for inference. Keep FP32 (your requirement).



* Worker pool: single main inference worker. Optionally 2nd low-priority worker for short cached answers.



* Queue: Redis Streams or RQ. Job payload: {user, convo\_id, message, top\_k\_retrieval\_ids, max\_tokens}.



* Streaming: model returns tokens; worker publishes token chunks to Redis pubsub; FastAPI subscribed and pushes to WebSocket.



* Memory guard: spawn model with ulimit/cgroups GPU memory limits if inside container and watch with nvidia-smi monitor script. On OOM, auto-restart worker and failover to small model.



* Max concurrency: 1 active inference job; allow N pending jobs (configurable, e.g., 50). If exceeded, return â€œsystem busy â€” try again or escalate.â€



Example minimal CLI-run pattern (conceptual):

\# model\_service: run once, create UNIX socket

PYTHONPATH=. python model\_service/run.py --socket /var/run/openthaigpt.sock --precision fp32







ğŸ“ŒRetrieval \& RAG (simple steps)



1. Preprocess docs: normalize Thai text (PyThaiNLP), split to 200â€“600 token chunks.



2\. Embeddings: local sentence-transformer (Thai capable).



3\. Index: FAISS flat or IVF+PQ (if many docs) with namespaces per bot. Also keep BM25 (Whoosh or ElasticLite) for lexical search.



4\. On query: do BM25 + vector retrieve â†’ hybrid score â†’ pick top N with threshold. If score < threshold, flag low-confidence and escalate. Always attach source snippets + links.









ğŸ“ŒSecurity, privacy, compliance (plain)



* TLS everywhere (Nginx).



* JWT short-lived for browser sessions + CSRF protections.



* PII redaction pipeline: before logging or indexing, detect phone numbers, IDs, emails â€” redact or mask; allow opt-out for transcript retention.



* Encryption at rest: Postgres and FAISS files encrypted (OS-level or filesystem encryption).



* Audit \& disclaimer: for legal topics show: â€œThis is informational only â€” not legal advice.â€ add consent checkbox on first use.







ğŸ“ŒCI/CD, testing \& release (best practice)



* Pre-commit (ruff, black, isort), mypy type-checks.



* Unit tests for retrieval, prompt templates, auth. Integration tests to simulate queueâ†’model (mock model).



* Docker multi-stage builds: keep CUDA runtime only in final image; build with base CUDA dev image but strip dev libs out. (You already follow this idea.)



* Canary \& rollout: test model changes in staging VM, then promote.







ğŸ“ŒMonitoring \& SLOs (what to track â€” simple list)

* request\_latency p50/p95/p99
* model\_inference\_time (per token \& total)
* queue\_length \& job\_age
* model\_memory\_util / OOM\_rate
* human\_escalation\_rate
* user\_feedback\_score

Alerts: queue\_length > X, OOM events, p99 latency > SLO.









ğŸ“ŒAdmin \& ops (day-to-day)



* Admin UI: reindex, edit docs, view bad responses, force-evict caches, trigger re-training candidate exports.



* Backups: nightly DB dumps + FAISS index snapshot + artifacts.



* Recovery plan: documented steps to bring model back (stop service, clear stale GPU memory, restart model service).



* Logs: structured JSON into file + ship to ELK/Sentry.



(Joke: If ops ever say â€œIt works on my machine,â€ send them to the GPU gym for a workout. ğŸ‹ï¸â€â™‚ï¸)















ğŸ“ŒFolder structure:



C:\\

â””â”€ Cblue\\

Â   â””â”€ customer\_support\\                     # MONOREPO ROOT (keep this in git)

Â      â”œâ”€â”€ .gitignore

Â      â”œâ”€â”€ README.md

Â      â”œâ”€â”€ LICENSE

Â      â”œâ”€â”€ .env.template                      # template for service envs (no secrets)

Â      â”œâ”€â”€ infra/                             # infra scripts \& helpers (not sensitive)

Â      â”‚   â”œâ”€â”€ setup\_dev\_env.ps1

Â      â”‚   â”œâ”€â”€ setup\_prod\_env.ps1

Â      â”‚   â”œâ”€â”€ backup\_faiss.ps1

Â      â”‚   â”œâ”€â”€ restore\_faiss.ps1

Â      â”‚   â””â”€â”€ ci\_build.sh

Â      â”‚

Â      â”œâ”€â”€ common/                            # shared libraries across bots

Â      â”‚   â”œâ”€â”€ nlp\_utils/

Â      â”‚   â”‚   â”œâ”€â”€ \_\_init\_\_.py

Â      â”‚   â”‚   â”œâ”€â”€ thai\_normalize.py

Â      â”‚   â”‚   â””â”€â”€ tokenizer.py

Â      â”‚   â”œâ”€â”€ retrieval/

Â      â”‚   â”‚   â”œâ”€â”€ \_\_init\_\_.py

Â      â”‚   â”‚   â”œâ”€â”€ hybrid\_search.py

Â      â”‚   â”‚   â””â”€â”€ index\_store.py

Â      â”‚   â”œâ”€â”€ prompting/

Â      â”‚   â”‚   â”œâ”€â”€ \_\_init\_\_.py

Â      â”‚   â”‚   â””â”€â”€ templates.py

Â      â”‚   â”œâ”€â”€ ocr/

Â      â”‚   â””â”€â”€ schemas/

Â      â”‚       â””â”€â”€ pydantic\_models.py

Â      â”‚

Â      â”œâ”€â”€ services/

Â      â”‚   â”œâ”€â”€ customer\_support\_bot/

Â      â”‚   â”‚   â”œâ”€â”€ backend/                     # FastAPI service (containerized)

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ app/

Â      â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ main.py

Â      â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router\_chat.py

Â      â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py

Â      â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ prompt\_templates.py

Â      â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ queue\_client.py

Â      â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py

Â      â”‚   â”‚   â”‚   â”‚   â””â”€â”€ schemas.py

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile.api

Â      â”‚   â”‚   â”‚   â””â”€â”€ docker-compose.yml       # service-level compose for local dev

Â      â”‚   â”‚   â”‚

Â      â”‚   â”‚   â”œâ”€â”€ model\_service/               # model process (recommended outside Docker or GPU-container)

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ run.py                   # starts model, listens on UNIX socket /var/run/openthaigpt.sock

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ model\_utils.py

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile.model        # optional: GPU container image if you prefer containerized model

Â      â”‚   â”‚   â”‚   â””â”€â”€ venv/                   # (if you run as venv on host; not committed)

Â      â”‚   â”‚   â”‚

Â      â”‚   â”‚   â”œâ”€â”€ frontend/                    # React chat widget (keep appearance unchanged)

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ widget/

Â      â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ src/

Â      â”‚   â”‚   â”‚   â”‚   â””â”€â”€ package.json

Â      â”‚   â”‚   â”‚   â””â”€â”€ admin\_ui/                # admin UI to reindex/manage docs

Â      â”‚   â”‚   â”‚

Â      â”‚   â”‚   â”œâ”€â”€ data/                        # DOCUMENTS / index sources (GIT-IGNORED)

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ website\_html/            # place scraped website HTML (git-ignored)

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ pdfs/

Â      â”‚   â”‚   â”‚   â””â”€â”€ sitemap.json

Â      â”‚   â”‚   â”‚

Â      â”‚   â”‚   â”œâ”€â”€ docker/                      # docker-compose for this service (production dev)

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ nginx/

Â      â”‚   â”‚   â”‚   â”‚   â””â”€â”€ conf.d/

Â      â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile.api -> ../../backend/Dockerfile.api

Â      â”‚   â”‚   â”‚   â””â”€â”€ volumes.env

Â      â”‚   â”‚   â”‚

Â      â”‚   â”‚   â””â”€â”€ scripts/

Â      â”‚   â”‚       â”œâ”€â”€ index\_docs.py            # build FAISS + BM25 index from data/

Â      â”‚   â”‚       â”œâ”€â”€ reindex\_watchdog.sh

Â      â”‚   â”‚       â””â”€â”€ export\_retrain\_candidates.sh

Â      â”‚   â”‚

Â      â”‚   â””â”€â”€ chatbot\_template/                # template for duplicating new bots (empty data/)

Â      â”‚

Â      â”œâ”€â”€ ops/                                # orchestration, monitoring, infra compose

Â      â”‚   â”œâ”€â”€ docker-compose.yml              # orchestrate nginx, db, redis, grafana, customer\_support\_backend

Â      â”‚   â”œâ”€â”€ prometheus/

Â      â”‚   â””â”€â”€ grafana/

Â      â”‚

Â      â””â”€â”€ docs/

Â          â”œâ”€â”€ architecture.md

Â          â”œâ”€â”€ runbook.md                       # recovery steps, OOM handling

Â          â”œâ”€â”€ monitoring.md

Â          â””â”€â”€ duplication\_guide.md             # step-by-step to clone to new server

